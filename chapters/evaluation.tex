\chapter{Evaluation}

The main reason for adding a compiler into a JavaScript runtime environment is a potential increase in execution speed. This chapter evaluates the performance difference between using Jaculus-machine with, and without the newly added compiler.

To measure the performance difference, we choose several benchmarks which aim to test different language constructs.

\section{Methodology}

Because Jaculus primarily targets resource-limited environments, the benchmarks were executed on a Raspberry Pi Zero 2 W~\cite{rpi0_datasheet} single board computer (SBC). The SBC is equipped with a 64-bit quad-core ARM Cortex-A53 CPU running at 1 GHz and 512 GB of RAM. The CPU is based on the ARMv8-A architecture and uses the AArch64 instruction set which is supported by the MIR backend used in the compiler.

A benchmarking executable was added to Jaculus-machine which allows the user to execute a JavaScript file in an interpreter or a compiler mode. The executable can specify the number of iterations to run, and can define global constants available from the interpreter to parameterize the benchmark. In each iteration, a new instance of the Jaculus Machine is created, and the benchmark is executed.

The time is measured from C++ code using \texttt{std::chrono} library and is split into two parts: time taken to initialize the runtime, and time taken to execute the benchmark. The final results are the average times over all iterations.

The code being run in the interpreter and compiler modes is not identical because the interpreter does not support the added type annotations which are, however, required by the compiler. The code for both benchmarks therefore contains small differences. The code for the compiler mode also uses the \texttt{Int32} type where possible to take advantage of integer operations being faster than floating-point operations.


\section{Benchmarks}

We have chosen three benchmarks to evaluate the performance of the compiler compared to only using the interpreter. The benchmarks are:
\begin{itemize}
    \item Naive recursive calculation of nth Fibonacci number (Figure \ref{fig:fibRec}),
    \item Matrix multiplication (Figure \ref{fig:matmul}), and
    \item Line rendering using Wu's algorithm (Figure \ref{fig:line}).
\end{itemize}

The first benchmark is a naive recursive calculation of the nth Fibonacci number. The benchmark primarily tests the performance of function calls, and not much computation is performed in the function itself. The number of calls in this benchmark is exponential in the value of \texttt{n}.

The second benchmark is a matrix multiplication of two matrices of a given size. The benchmark mainly tests the performance of loops, array access, and arithmetic operations. The number of operations in this benchmark is quadratic in the size of the matrices.

The third benchmark renders 100 lines using Wu's algorithm~\cite{wu}. The benchmark tests the performance of loops, array access, and arithmetic operations. The number of operations in this benchmark is linear in the length of the lines.

We have also compared the performance of the compiler when using \texttt{Int32} operations and \texttt{Float64} operations. The comparison was performed on the first benchmark -- Fibonacci.

Source code for all benchmarks is available in the attached archive described in Appendix \ref{app:attachments}.


\subsection{Results}

The execution times of the benchmarks are shown in Figures \ref{fig:fibRec}, \ref{fig:matmul}, and \ref{fig:line}. Average initialization times are shown in Table \ref{tab:init}.

The results show that in compiler mode, the runtime environment outperforms the interpreter mode for larger inputs in all benchmarks. For large inputs, the performance difference in these benchmarks is by a factor of 6 to 14.

The downside of the compiler is a slower startup time. It is best visible in the Fibonacci benchmark where the startup time dominates the execution time for inputs smaller than 23. The effect is less pronounced in the other benchmarks, because their computational intensity increases faster even for smaller inputs. However, because the compiler is primarily intended for longer running programs, this will likely not affect the real performance much.

There is only a small difference in performance between using \texttt{Int32} and \texttt{Float64} in the Fibonacci benchmark. The difference is around 20\% for larger inputs. The reason for this result is that the used CPU has a 64-bit floating point unit (FPU) which is used to accelerate floating point operations. If the benchmark was run on a microcontroller without an FPU, the performance difference would likely be much larger.


\begin{table}[!b]
    \centering
    \begin{tabular}{|l|l|l|}
        \hline
        Benchmark             & Interpreter (ms) & Compiler (ms) \\\hline
        Fibonacci             & 1.625            & 7.787         \\
        Matrix multiplication & 2.088            & 13.766        \\
        Line rendering        & 2.226            & 35.180        \\
        \hline
    \end{tabular}
    \caption{Average initialization times of the runtime.}
    \label{tab:init}
\end{table}

\pagebreak

\pgfplotsset{width=1\textwidth,height=0.45\textwidth}

\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            ymode=log,
            log basis y=10,
            log ticks with fixed point,
            legend pos=north west,
            xlabel={Input size ($n$)},
            ylabel={Execution time (ms, log scale)}
        ]
            \addplot[blue, mark=*] table [x=-Dx,y=run_time] {assets/out_pi/bench_processed/fibRec/interp.csv};
            \addlegendentry{Interpreter}
            \addplot[gray, mark=square*] table [x=-Dx,y=run_time] {assets/out_pi/bench_processed/fibRec/aotFloat.csv};
            \addlegendentry{Compiler (Float64)}
            \addplot[red, mark=triangle*] table [x=-Dx,y=run_time] {assets/out_pi/bench_processed/fibRec/aot.csv};
            \addlegendentry{Compiler (Int32)}
        \end{axis}
    \end{tikzpicture}
    \caption{Results of the Fibonacci benchmark.}
    \label{fig:fibRec}
\end{figure}

\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            ytick distance=100,
            legend pos=north west,
            xlabel={Matrix dimension},
            ylabel={Execution time (ms)}
        ]
            \addplot[blue, mark=*] table [x=-Dx,y=run_time] {assets/out_pi/bench_processed/matMul/interp.csv};
            \addlegendentry{Interpreter}
            \addplot[red, mark=triangle*] table [x=-Dx,y=run_time] {assets/out_pi/bench_processed/matMul/aot.csv};
            \addlegendentry{Compiler}
        \end{axis}
    \end{tikzpicture}
    \caption{Results of the matrix multiplication benchmark.}
    \label{fig:matmul}
\end{figure}

\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            ytick distance=100,
            legend pos=north west,
            xlabel={Frame dimension},
            ylabel={Execution time (ms)}
        ]
            \addplot[blue, mark=*] table [x=-Ds,y=run_time] {assets/out_pi/bench_processed/graphics/interp.csv};
            \addlegendentry{Interpreter}
            \addplot[red, mark=triangle*] table [x=-Ds,y=run_time] {assets/out_pi/bench_processed/graphics/aot.csv};
            \addlegendentry{Compiler}
        \end{axis}
    \end{tikzpicture}
    \caption{Results of the line rendering benchmark.}
    \label{fig:line}
\end{figure}
